{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 99.64%\n",
      "\n",
      "Detailed Classification Report:\n",
      "------------------------------------------------------------------\n",
      "Class         | Precision | Recall | F1-Score | Support\n",
      "------------------------------------------------------------------\n",
      "Loss          | 1.00      | 1.00   | 1.00    | 112\n",
      "Draw          | 0.98      | 1.00   | 0.99    | 59\n",
      "Win           | 1.00      | 0.99   | 1.00    | 107\n",
      "------------------------------------------------------------------\n",
      "Overall Accuracy: 99.64%\n",
      "------------------------------------------------------------------\n",
      "\n",
      "* Precision: The ability of the model to correctly classify a class among all predicted instances of that class.\n",
      "* Recall: The ability of the model to find all instances of a particular class.\n",
      "* F1-Score: The weighted average of Precision and Recall.\n",
      "* Support: The number of actual occurrences for each class in the test set.\n",
      "\n",
      "Sample Predictions (Actual vs Predicted):\n",
      "------------------------------------------------\n",
      "Index | Actual Result | Predicted Result\n",
      "------------------------------------------------\n",
      "  136 | Win           | Win\n",
      "   39 | Loss          | Loss\n",
      "   25 | Loss          | Loss\n",
      "  248 | Win           | Win\n",
      "  162 | Win           | Win\n",
      "   66 | Loss          | Loss\n",
      "  184 | Loss          | Loss\n",
      "  130 | Loss          | Loss\n",
      "  195 | Loss          | Loss\n",
      "  148 | Loss          | Loss\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"matches.csv\", index_col=0)\n",
    "\n",
    "# Check and drop unnecessary columns if they exist\n",
    "columns_to_drop = [col for col in ['Unnamed: 0', 'match report', 'notes', 'date', 'time', 'comp', 'round', 'referee'] if col in df.columns]\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Convert 'result' to numerical labels: Win = 1, Draw = 0, Loss = -1\n",
    "df_cleaned['result'] = df_cleaned['result'].map({'W': 1, 'D': 0, 'L': -1})\n",
    "\n",
    "# Handle missing values only for numeric columns\n",
    "numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_cleaned[numeric_cols] = df_cleaned[numeric_cols].fillna(df_cleaned[numeric_cols].mean())\n",
    "\n",
    "# Feature Engineering: Create goal difference, shot accuracy\n",
    "df_cleaned['goal_difference'] = df_cleaned['gf'] - df_cleaned['ga']\n",
    "df_cleaned['shot_accuracy'] = df_cleaned['sot'] / df_cleaned['sh']\n",
    "\n",
    "# Drop the original 'gf' and 'ga' columns as they've been incorporated into goal difference\n",
    "df_cleaned = df_cleaned.drop(columns=['gf', 'ga'])\n",
    "\n",
    "# One-hot encoding of categorical features\n",
    "df_encoded = pd.get_dummies(df_cleaned, columns=['day', 'venue', 'opponent', 'formation', 'captain', 'team'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_encoded.drop(columns=['result'])\n",
    "y = df_encoded['result']\n",
    "\n",
    "# Impute any remaining missing values in the features before splitting\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed_full = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed_full, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Map numerical predictions back to string labels for better readability\n",
    "label_mapping = {1: 'Win', 0: 'Draw', -1: 'Loss'}\n",
    "y_test_labels = y_test.map(label_mapping).reset_index(drop=True)\n",
    "y_pred_labels = pd.Series(y_pred).map(label_mapping).reset_index(drop=True)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=['Loss', 'Draw', 'Win'], output_dict=True)\n",
    "\n",
    "# Display the accuracy and a formatted classification report\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2%}\\n\")\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Class         | Precision | Recall | F1-Score | Support\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "for label, metrics in classification_rep.items():\n",
    "    if label in ['Loss', 'Draw', 'Win']:\n",
    "        print(f\"{label:<13} | {metrics['precision']:.2f}      | {metrics['recall']:.2f}   | {metrics['f1-score']:.2f}    | {int(metrics['support'])}\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(f\"Overall Accuracy: {classification_rep['accuracy']:.2%}\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"\\n* Precision: The ability of the model to correctly classify a class among all predicted instances of that class.\")\n",
    "print(\"* Recall: The ability of the model to find all instances of a particular class.\")\n",
    "print(\"* F1-Score: The weighted average of Precision and Recall.\")\n",
    "print(\"* Support: The number of actual occurrences for each class in the test set.\\n\")\n",
    "\n",
    "# Format the sample predictions in a similar table style\n",
    "print(\"Sample Predictions (Actual vs Predicted):\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Index | Actual Result | Predicted Result\")\n",
    "print(\"------------------------------------------------\")\n",
    "sample_results = results_df.sample(10).reset_index()  # Display 10 random sample predictions\n",
    "\n",
    "for _, row in sample_results.iterrows():\n",
    "    print(f\"{row['index']:>5} | {row['Actual Result']:<13} | {row['Predicted Result']}\")\n",
    "\n",
    "print(\"------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
